{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c0f0fd4",
   "metadata": {},
   "source": [
    "# IML Take-Home Assignment 3\n",
    "\n",
    "_Your names here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad9fb8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Problem 1\n",
    "> Consider the following interface for machine learning \"modules\" that can be chained together in series. (If you like, you can return to this code later and add features like debug information / hooks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe3a0f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def __call__(self, *args, **kwargs) -> np.ndarray:\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.x = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, dy: np.ndarray) -> np.ndarray:\n",
    "        return dy\n",
    "\n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *components: Module):\n",
    "        self.components: tuple[Module, ...] = components\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c in self.components:\n",
    "            x = c(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dy):\n",
    "        for c in reversed(self.components):\n",
    "            dy = c.backward(dy)\n",
    "        return dy\n",
    "\n",
    "    def step(self):\n",
    "        for c in self.components:\n",
    "            c.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753e0ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "> **Fill in the following implementation of a \"linear layer.\"** In writing, explain the meaning of the \"backward pass\" and how you derived the expressions involved. _(3 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25378b11",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Linear(Module):\n",
    "    d_in: int\n",
    "    d_out: int\n",
    "    init_scale: float = 1\n",
    "    learning_rate: float = 1\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.weights = self.init_scale * randn(self.d_in, self.d_out)\n",
    "        self.grad = np.zeros_like(self.weights)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.x = x\n",
    "        return x @ self.weights\n",
    "\n",
    "    def backward(self, dy: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def step(self):\n",
    "        self.weights -= self.learning_rate * self.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72f382",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Problem 2\n",
    "> The following class wlll train a model on a synthetic regression problem and display the learned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d9ad3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class RegressionProblem:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Module,\n",
    "        fn: Callable[[np.ndarray], np.ndarray],\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.b = 50\n",
    "        self.x = np.linspace(0, 1, self.b)[:, None]  # shape: (b, 1)\n",
    "        self.y = fn(self.x) + 0.1 * randn(self.b, 1)  # shape: (b, 1)\n",
    "\n",
    "    def train(self, iters=10) -> pd.Series:\n",
    "        losses = np.zeros(iters)\n",
    "        for i in range(iters):\n",
    "            y_hat = self.model(self.x)\n",
    "            losses[i] = ((y_hat - self.y) ** 2).sum(axis=1).mean()\n",
    "            self.model.backward((y_hat - self.y) / self.b)\n",
    "            self.model.step()\n",
    "        series = pd.Series(losses, name=\"Mean squared error\")\n",
    "        series.index.name = \"Iteration\"\n",
    "        return series\n",
    "\n",
    "    def show(self):\n",
    "        df = pd.DataFrame({\"x\": self.x.flatten(), \"y\": self.y.flatten()})\n",
    "        sns.scatterplot(df, x=\"x\", y=\"y\")\n",
    "        x_sample = np.linspace(-0.2, 1.2, 100)[:, None]\n",
    "        plt.plot(x_sample, self.model(x_sample))\n",
    "\n",
    "\n",
    "class AffineEmbed(Module):\n",
    "    def forward(self, x):\n",
    "        return np.concat([np.ones((x.shape[0], 1)), x], axis=1)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dy[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a635ed0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "> Using your implementation of `Linear` above, **define a `Sequential` model that learns a function of the form $y = a x + b$** by applying a suitable \"embedding\" before your linear layer. (Define a module to perform your embedding.) Does the embedding need a backward pass? _(3 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbf729",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model = Sequential(\n",
    "    # <- Add an embedding here?\n",
    "    Linear(1, 1),  # A learnable linear map\n",
    ")\n",
    "problem = RegressionProblem(model, lambda x: x + 1)\n",
    "# problem.train(100)\n",
    "problem.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea22ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Problem 3\n",
    "> Now, let's solve a non-linear regression problem using a very simple neural network with a fixed (untrainable) hidden layer.\n",
    ">\n",
    "> Let $(r_i)_{i = 1}^d$ be a sequence of $d$ \"reference inputs\" and like $x$ be the input to the network. For each $i = 1, \\dots, n,$ let the hidden neuron $h_i$ equal $1$ if $r_i$ is the reference input closest to $x$ and $0$ otherwise. Finally, let the output $y$ of the network be a linear combination\n",
    "> $$\n",
    "y = \\sum_{i = 1}^n w_i h_i.\n",
    "$$\n",
    ">\n",
    "> The following code implements such a network. **Analytically derive the optimal weights $w_i$ for this network under a squared error loss.** What is this network really learning? _(3 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf71d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEmbedding(Module):\n",
    "    def __init__(self, references: np.ndarray):\n",
    "        self.references = references  # (d_embed, d_in)\n",
    "        self.d_embed, self.d_in = self.references.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        out = np.zeros((b, self.d_embed))\n",
    "        distances = np.sum((self.references[None] - x[:, None, :]) ** 2, axis=-1)\n",
    "        out[np.arange(b), np.argmin(distances, axis=-1)] = 1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bc15d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model = Sequential(\n",
    "    LocalEmbedding(np.linspace(0, 1, 8)[:, None]),\n",
    "    Linear(8, 1),\n",
    ")\n",
    "problem = RegressionProblem(model, lambda x: np.sin(3 * x) + 1)\n",
    "# problem.train(50)\n",
    "problem.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d784f16f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Problem 4\n",
    "> Now we'll make the hidden neurons learnable. Each hidden neuron $h_i$ should compute $\\sigma(a_i x + b_i),$ where $\\sigma$ is the \"logistic sigmoid function\" $\\sigma(x) = 1 / (1 + e^{-x})$  and $(a_i, b_i)$ are learnable parameters. As before, the output $y$ will be a learnable linear function of the hidden neurons $h_i.$\n",
    ">\n",
    "> **Implement and train this two-layer MLP on the same problem as above.** Use 16 hidden neurons. Find hyperparameters that make the network converge to a reasonable model in only 200 iterations. Graph the function learned by the network. _(5 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, dy):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5716a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def make_simple_mlp(d_embed=16):\n",
    "    return Sequential(\n",
    "        # <- Something here?\n",
    "        Linear(1, d_embed, init_scale=1, learning_rate=d_embed),\n",
    "        # <- Something here?\n",
    "        Linear(d_embed, 1, init_scale=1 / d_embed, learning_rate=1 / d_embed),\n",
    "    )\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "problem = RegressionProblem(make_simple_mlp(), lambda x: np.sin(3 * x) + 1)\n",
    "# problem.train(200)\n",
    "problem.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419883d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Problem 5\n",
    "> The following class will train a model on a synthetic classification problem and display the learned classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc08ed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def make_cluster(mean, n_points: int, label: int):\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"x\": mean[0] + 0.2 * randn(n_points),\n",
    "            \"y\": mean[1] + 0.2 * randn(n_points),\n",
    "            \"label\": label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "class ClassificationProblem:\n",
    "    def __init__(self, model: Module):\n",
    "        self.model = model\n",
    "        self.n_clusters = 3\n",
    "        self.df = pd.concat(\n",
    "            [\n",
    "                make_cluster([1, 1], 50, 0),\n",
    "                make_cluster([-1, 1], 50, 1),\n",
    "                make_cluster([-1, -1], 50, 0),\n",
    "                make_cluster([1, -1], 50, 1),\n",
    "                make_cluster([0, 0], 50, 2),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    def train(self, n_iters=100):\n",
    "        loss = np.zeros(n_iters)\n",
    "        for i in tqdm(range(n_iters)):\n",
    "            self.model(self.df[[\"x\", \"y\"]].to_numpy())\n",
    "            self.model.backward(self.df[\"label\"].to_numpy())\n",
    "            loss[i] = self.model.components[-1].loss\n",
    "            self.model.step()\n",
    "        return loss\n",
    "\n",
    "    def show(self, palette: str = \"bright\"):\n",
    "        from matplotlib.colors import ListedColormap\n",
    "\n",
    "        extent = (-2, 2, -2, 2)\n",
    "\n",
    "        x, y = np.meshgrid(\n",
    "            np.arange(*extent[:2], step=0.01),\n",
    "            np.arange(*extent[2:], step=0.01),\n",
    "        )\n",
    "        colors = sns.color_palette(palette, n_colors=self.n_clusters)\n",
    "        out = self.model(np.stack([arr.flatten() for arr in [x, y]], axis=1))\n",
    "        plt.matshow(\n",
    "            out.reshape(x.shape)[::-1, :],\n",
    "            extent=extent,\n",
    "            alpha=0.3,\n",
    "            cmap=ListedColormap(colors),\n",
    "        )\n",
    "        sns.scatterplot(self.df, x=\"x\", y=\"y\", hue=\"label\", palette=palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2364006",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "> **Build a two-layer MLP** and train it on this problem. Requirements:\n",
    ">\n",
    "> - Use the ReLU activation function for the hidden layer.\n",
    "> - Use a softmax on the output layer to generate a vector of probabilities.\n",
    "> - Train the network with cross-entropy loss.\n",
    ">\n",
    "> `ClassificationProblem.train()` expects that the backwards pass of your `Module` accepts a vector of true classes, and that its last layer computes a `loss` attribute. Feel free to edit this if you want. _(6 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8de477",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CrossEntropyLoss(Module):\n",
    "    n_classes: int\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.argmax(x, axis=1)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # dy is a vector of true classes\n",
    "        self.loss = NotImplementedError()\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "def make_classifier():\n",
    "    # Replace these layers\n",
    "    return Sequential(\n",
    "        AffineEmbed(),\n",
    "        Linear(3, 4),\n",
    "        CrossEntropyLoss(n_classes=4),\n",
    "    )\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "problem = ClassificationProblem(model=make_classifier())\n",
    "# problem.train(100)\n",
    "problem.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3856b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Problem 6 (4 points)\n",
    "> Build a neural network that can learn addition modulo 10. The architecture is up to you!\n",
    ">\n",
    "> Requirements:\n",
    ">\n",
    "> - _Do not_ encode numbers directly as input values. Instead, use one-hot embeddings or randomly initialized, learnable embeddings.\n",
    "> - Reserve a very small fraction of data points to compute test loss.\n",
    "> - Graph test loss and training loss over many epochs of full-batch gradient descent.\n",
    ">\n",
    "> For full points, test loss should drop to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eacb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addition_dataset(n=10):\n",
    "    values = np.arange(n)\n",
    "    x, y = [arr.flatten() for arr in np.meshgrid(values, values)]\n",
    "    return pd.DataFrame({\"x\": x, \"y\": y, \"z\": (x + y) % n})\n",
    "\n",
    "\n",
    "sns.heatmap(addition_dataset().pivot(index=\"x\", columns=\"y\", values=\"z\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe3a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
